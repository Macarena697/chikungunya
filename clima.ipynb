{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79a312f-b5de-46b1-8a9e-51e8d9346ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install meteostat tqdm\n",
    "#%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3706688e-c2a6-4e20-be69-3048566e7758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, DateType, StringType\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from meteostat import Point, Daily, Stations\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e4dde9-fa9c-405b-a94d-d2d62c11c877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- CARGAR DATOS INICIALES ---\n",
    "spark = SparkSession.builder.appName(\"ClimaEnrichment\").getOrCreate()\n",
    "df_con_geo = spark.table(\"workspace.default.union_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2467dd72-c0ba-41b1-ac98-801ee1dc63f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "METEOSTAT_API_KEY = \"8a4b224e05msh7db26cbb8f4532bp1b931djsn4a9215830d1a\"\n",
    "Daily.key = METEOSTAT_API_KEY\n",
    "Stations.key = METEOSTAT_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d02c38b-d269-4ef9-8563-a19c9eabd87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Llamadas a la API ---\n",
    "loc_dates_unicas_pd = df_con_geo.select(\"lat_municipio\", \"lon_municipio\", \"fecha_notificacion\").distinct().toPandas().dropna()\n",
    "\n",
    "loc_unicas = loc_dates_unicas_pd[['lat_municipio', 'lon_municipio']].drop_duplicates().to_records(index=False)\n",
    "estaciones_map = {}\n",
    "loc_sin_estacion = []\n",
    "for lat, lon in tqdm(loc_unicas, desc=\"Buscando estaciones cercanas\"):\n",
    "    try:\n",
    "        stations = Stations().nearby(lat, lon).fetch(1)\n",
    "        if not stations.empty:\n",
    "            estaciones_map[(lat, lon)] = stations.index[0]\n",
    "        else:\n",
    "            loc_sin_estacion.append((lat, lon))\n",
    "    except Exception:\n",
    "        loc_sin_estacion.append((lat, lon))\n",
    "\n",
    "mapa_pd = pd.DataFrame(estaciones_map.items(), columns=['coords', 'station_id'])\n",
    "mapa_pd[['lat_municipio', 'lon_municipio']] = pd.DataFrame(mapa_pd['coords'].tolist(), index=mapa_pd.index)\n",
    "df_con_estaciones = pd.merge(loc_dates_unicas_pd, mapa_pd, on=['lat_municipio', 'lon_municipio'], how='left').dropna(subset=['station_id'])\n",
    "consultas_por_estacion = df_con_estaciones.groupby('station_id')['fecha_notificacion'].agg(list)\n",
    "\n",
    "lista_clima_pd = []\n",
    "for station_id, fechas in tqdm(consultas_por_estacion.items(), desc=\"Descargando datos climáticos\"):\n",
    "    try:\n",
    "        start_date = datetime(min(fechas).year, min(fechas).month, min(fechas).day)\n",
    "        end_date = datetime(max(fechas).year, max(fechas).month, max(fechas).day)\n",
    "        data = Daily(station_id, start_date, end_date).fetch()\n",
    "        if not data.empty:\n",
    "            data['station_id'] = station_id\n",
    "            lista_clima_pd.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Advertencia: No se pudieron obtener datos para la estación {station_id}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ccbeed-22cf-4cbd-b462-4a6ee9625399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Unir datos climáticos y guardar tabla final ---\n",
    "if not lista_clima_pd:\n",
    "    print(\"Error: No se pudo obtener ningún dato climático de la API. El proceso se detendrá.\")\n",
    "else:\n",
    "    clima_pd_completo = pd.concat(lista_clima_pd).reset_index()[['time', 'station_id', 'tavg', 'prcp']]\n",
    "    clima_pd_completo.rename(columns={'time': 'fecha_notificacion', 'tavg': 'temperatura_media', 'prcp': 'precipitacion'}, inplace=True)\n",
    "    \n",
    "    mapa_estaciones_spark = spark.createDataFrame(mapa_pd)\n",
    "    df_con_station_id = df_con_geo.join(mapa_estaciones_spark, on=['lat_municipio', 'lon_municipio'], how='left')\n",
    "    \n",
    "    clima_spark_df = spark.createDataFrame(clima_pd_completo)\n",
    "    df_final_con_clima = df_con_station_id.join(clima_spark_df, on=['station_id', 'fecha_notificacion'], how='left')\n",
    "    \n",
    "    print(\"Guardando la tabla enriquecida en 'workspace.default.complete_dataset_con_clima'...\")\n",
    "    df_final_con_clima.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"workspace.default.complete_dataset_con_clima\")    \n",
    "    \n",
    "    count_final = df_final_con_clima.count()\n",
    "    print(f\"--- Proceso completado. Se guardaron {count_final} registros en 'complete_dataset_con_clima'. ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clima",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
