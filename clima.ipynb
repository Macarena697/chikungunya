{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79a312f-b5de-46b1-8a9e-51e8d9346ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# OBJETIVO: Enriquecer o conjunto de dados de casos com dados climáticos (temperatura e\n",
    "#           precipitação) obtidos de uma API externa (Meteostat) de acordo com a\n",
    "#           localização e data de cada notificação.\n",
    "# ======================================================================================\n",
    "\n",
    "# --- 0. INSTALAÇÃO DE BIBLIOTECAS (se necessário) ---\n",
    "# Estas linhas instalam as bibliotecas necessárias para a execução do script.\n",
    "# meteostat: para consultar a API de dados climáticos.\n",
    "# tqdm: para exibir barras de progresso em processos longos.\n",
    "%pip install meteostat tqdm\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3706688e-c2a6-4e20-be69-3048566e7758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURAÇÃO E INICIALIZAÇÃO ---\n",
    "\n",
    "# Importação das bibliotecas e funções necessárias.\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, DateType, StringType\n",
    "import pandas as pd\n",
    "from datetime import datetime, date \n",
    "from meteostat import Point, Daily, Stations\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96e4dde9-fa9c-405b-a94d-d2d62c11c877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializa a sessão do Spark.\n",
    "spark = SparkSession.builder.appName(\"EnriquecimentoClima\").getOrCreate()\n",
    "\n",
    "# Carrega a tabela previamente processada que contém os dados dos casos,\n",
    "# incluindo as coordenadas geográficas (latitude e longitude) de cada município.\n",
    "df_com_geo = spark.table(\"workspace.default.casos_unificados_br\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2467dd72-c0ba-41b1-ac98-801ee1dc63f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se configura la clave de la API para la librería meteostat.\n",
    "# Es necesario para realizar consultas a su servicio.\n",
    "METEOSTAT_API_KEY = \"8a4b224e05msh7db26cbb8f4532bp1b931djsn4a9215830d1a\"\n",
    "Daily.key = METEOSTAT_API_KEY\n",
    "Stations.key = METEOSTAT_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d02c38b-d269-4ef9-8563-a19c9eabd87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. OBTENÇÃO DE ESTAÇÕES CLIMÁTICAS PRÓXIMAS ---\n",
    "\n",
    "# Para otimizar as chamadas à API, primeiro identificam-se as combinações únicas\n",
    "# de localização (lat, lon) e data de notificação em todo o conjunto de dados.\n",
    "# .toPandas() converte o resultado do Spark para um DataFrame do Pandas,\n",
    "# já que a biblioteca meteostat opera sobre estruturas de dados do Pandas.\n",
    "loc_datas_unicas_pd = df_com_geo.select(\"lat_municipio\", \"lon_municipio\", \"data_notificacao\").distinct().toPandas().dropna()\n",
    "\n",
    "# Das combinações únicas, extraem-se apenas as coordenadas geográficas únicas.\n",
    "# O objetivo é encontrar uma estação climática por cada localização, não por cada data.\n",
    "loc_unicas = loc_datas_unicas_pd[['lat_municipio', 'lon_municipio']].drop_duplicates().to_records(index=False)\n",
    "\n",
    "# Dicionário para mapear cada coordenada (lat, lon) à sua estação mais próxima.\n",
    "mapa_estacoes = {}\n",
    "# Lista para guardar as localizações para as quais não foi encontrada uma estação.\n",
    "loc_sem_estacao = []\n",
    "\n",
    "# Itera sobre cada localização geográfica única para encontrar sua estação mais próxima.\n",
    "# 'tqdm' exibe uma barra de progresso, útil para monitorar processos longos.\n",
    "for lat, lon in tqdm(loc_unicas, desc=\"Buscando estações próximas\"):\n",
    "    try:\n",
    "        stations = Stations().nearby(lat, lon).fetch(1)\n",
    "        if not stations.empty:\n",
    "            mapa_estacoes[(lat, lon)] = stations.index[0]\n",
    "        else:\n",
    "            loc_sem_estacao.append((lat, lon))\n",
    "    except Exception:\n",
    "        loc_sem_estacao.append((lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ccbeed-22cf-4cbd-b462-4a6ee9625399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. AGRUPAMENTO DE CONSULTAS E DOWNLOAD DE DADOS CLIMÁTICOS ---\n",
    "\n",
    "# Converte o mapa de estações em um DataFrame para facilitar a junção.\n",
    "mapa_pd = pd.DataFrame(mapa_estacoes.items(), columns=['coords', 'station_id'])\n",
    "mapa_pd[['lat_municipio', 'lon_municipio']] = pd.DataFrame(mapa_pd['coords'].tolist(), index=mapa_pd.index)\n",
    "\n",
    "# Junta as datas de notificação com as estações encontradas.\n",
    "df_com_estacoes = pd.merge(loc_datas_unicas_pd, mapa_pd, on=['lat_municipio', 'lon_municipio'], how='left').dropna(subset=['station_id'])\n",
    "\n",
    "# Agrupa as datas por estação para fazer uma única chamada de API por estação.\n",
    "consultas_por_estacao = df_com_estacoes.groupby('station_id')['data_notificacao'].agg(list)\n",
    "\n",
    "# Lista para armazenar os DataFrames de dados climáticos obtidos.\n",
    "lista_clima_pd = []\n",
    "hoje = date.today()\n",
    "\n",
    "for station_id, datas in tqdm(consultas_por_estacao.items(), desc=\"Baixando dados climáticos\"):\n",
    "    try:\n",
    "        data_inicio = min(datas)\n",
    "        data_fim = max(datas)\n",
    "        \n",
    "        # Lógica simplificada: Se a data final for no futuro, ajuste-a para hoje.\n",
    "        if data_fim > hoje:\n",
    "            data_fim = hoje\n",
    "        \n",
    "        # Apenas prossiga se o intervalo de datas for válido.\n",
    "        if data_inicio <= data_fim:\n",
    "            data = Daily(station_id, data_inicio, data_fim).fetch()\n",
    "            \n",
    "            if not data.empty:\n",
    "                data['station_id'] = station_id\n",
    "                lista_clima_pd.append(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Aviso: Não foi possível obter dados para a estação {station_id} no intervalo de {data_inicio} a {data_fim}. Erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbc7bda7-1813-4609-a9b1-ab107b082c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. JUNÇÃO DOS DADOS CLIMÁTICOS E GRAVAÇÃO DA TABELA FINAL ---\n",
    "\n",
    "if not lista_clima_pd:\n",
    "    print(\"Erro: Não foi possível obter nenhum dado climático da API. O processo será interrompido.\")\n",
    "else:\n",
    "    # Concatena todos os resultados climáticos em um único DataFrame do Pandas.\n",
    "    clima_pd_completo = pd.concat(lista_clima_pd).reset_index()[['time', 'station_id', 'tavg', 'prcp']]\n",
    "    clima_pd_completo.rename(columns={'time': 'data_notificacao', 'tavg': 'temperatura_media', 'prcp': 'precipitacao'}, inplace=True)\n",
    "    \n",
    "    # Converte o mapa de estações para um DataFrame do Spark para a junção.\n",
    "    mapa_estacoes_spark = spark.createDataFrame(mapa_pd)\n",
    "    # Adiciona o ID da estação ao DataFrame principal de casos.\n",
    "    df_com_station_id = df_com_geo.join(mapa_estacoes_spark, on=['lat_municipio', 'lon_municipio'], how='left')\n",
    "    \n",
    "    # Converte os dados climáticos para um DataFrame do Spark.\n",
    "    clima_spark_df = spark.createDataFrame(clima_pd_completo)\n",
    "    \n",
    "    # Junta os dados climáticos ao DataFrame principal usando o ID da estação e a data da notificação.\n",
    "    df_final_com_clima = df_com_station_id.join(clima_spark_df, on=['station_id', 'data_notificacao'], how='left')\n",
    "    \n",
    "    print(\"Salvando a tabela enriquecida em 'workspace.default.casos_completos_com_clima'...\")\n",
    "    df_final_com_clima.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"workspace.default.casos_completos_com_clima\")    \n",
    "    \n",
    "    count_final = df_final_com_clima.count()\n",
    "    print(f\"--- Processo concluído. Foram salvos {count_final} registros em 'casos_completos_com_clima'. ---\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clima",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
